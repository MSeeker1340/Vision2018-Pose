{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing module\n",
    "## Loads COCO images and annotations and perform necessary preprocessing.\n",
    "## Since the individual datasets are <20GB (smaller after filtering & resizing),\n",
    "## we opt to simply store everything in an ndarray. If more data is present\n",
    "## we should probably use the `tf.data` API.\n",
    "\n",
    "import numpy as np\n",
    "from warnings import warn\n",
    "from os import path\n",
    "from skimage import img_as_float\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from pycocotools.coco import COCO\n",
    "from config import NUM_KEYPOINTS, image_shape, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_confidence_map(img_shape, Y, keypoints, sigma, image_path):\n",
    "    \"\"\"\n",
    "    Updates the w' x h' x NUM_KEYPOINTS confidence maps Y using one person's\n",
    "    keypoints data.\n",
    "    \"\"\"\n",
    "    # Safety check\n",
    "    Y_tmp = np.zeros((img_shape[0], img_shape[1], NUM_KEYPOINTS))\n",
    "    if len(keypoints) != 3 * NUM_KEYPOINTS:\n",
    "        warn(f\"Keypoints data for {image_path} is corrupted.\")\n",
    "        return\n",
    "    for k in range(NUM_KEYPOINTS):\n",
    "        x, y, visibility = keypoints[3*k], keypoints[3*k+1], keypoints[3*k+2]\n",
    "        if visibility == 2: # labeled and visible\n",
    "            # Scale the coordinates\n",
    "            # x /= scale[0]\n",
    "            # y /= scale[1]\n",
    "            # Calculate using a Gaussian kernel and take max\n",
    "            # TODO: vectorize this part\n",
    "            for i in range(Y.shape[0]):\n",
    "                for j in range(Y.shape[1]):\n",
    "                    Y_tmp[i,j,k] = max(Y_tmp[i,j,k], np.exp(-((i - x)**2 + (j - y)**2) / sigma**2))\n",
    "    \n",
    "    Y = resize(Y_tmp, Y.shape, mode='reflect', anti_aliasing=True)         \n",
    "                    \n",
    "\n",
    "def load_data(data_dir, data_type, image_shape=image_shape, sigma=sigma, num_input=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Load raw data from disk and preprocess into feature and label tensors.\n",
    "\n",
    "    - data_dir: path to COCO dataset\n",
    "\n",
    "    - data_type: 'train2014', 'val2014', 'test2014' or 'test2015'\n",
    "\n",
    "    - image_shape: standardized shape to resize raw images into. Default: (224,224).\n",
    "\n",
    "    - sigma: width of the Gaussian kernel used in the construction of the ground truth\n",
    "      confidence map. Default: 1.0.\n",
    "\n",
    "    - num_input: how many inputs to use, or all if set to None. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load metadata\n",
    "    image_dir = path.join(data_dir, \"images\", data_type)\n",
    "    anno_path = path.join(data_dir, \"annotations\", f\"person_keypoints_{data_type}.json\")\n",
    "    coco = COCO(anno_path)\n",
    "    image_ids = coco.getImgIds(\n",
    "        catIds=coco.getCatIds(catNms=['person'])\n",
    "    ) # only load person images\n",
    "    image_ids = np.random.permutation(image_ids)\n",
    "    if num_input != None:\n",
    "        image_ids = image_ids[:num_input]\n",
    "    images = coco.loadImgs(image_ids)\n",
    "\n",
    "    # Allocate feature and label tensor\n",
    "    ## For the confidence map, here we hard-code its shape as 1/8 of the input shape\n",
    "    ## (as a result of three pooling layers in Vgg19).\n",
    "    if image_shape[0] % 8 != 0 or image_shape[1] % 8 != 0:\n",
    "        warn(\"Input shape not divisible by 8.\")\n",
    "    confmap_shape = (image_shape[0]//8, image_shape[1]//8)\n",
    "    X = np.ndarray((len(images), *image_shape, 3))\n",
    "    Y = np.zeros((len(images), *confmap_shape, NUM_KEYPOINTS)) # one confidence map for each keypoint\n",
    "\n",
    "    # Build the tensors\n",
    "    for i, img_data in enumerate(images):\n",
    "        # Build X (feature; scaled and resized input image)\n",
    "        img_path = path.join(image_dir, img_data['file_name'])\n",
    "        verbose and print(\"Processing \", img_path)\n",
    "        img = img_as_float(imread(img_path)) # scale pixel value to [0,1]\n",
    "        if len(img.shape) == 2:\n",
    "            # Handle black and white images\n",
    "            img = np.tile(np.reshape(img, (*img.shape, 1)), 3)\n",
    "        X[i,:,:,:] = resize(img, image_shape, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "        # Compute the scale between the raw image and the confidence maps.\n",
    "        # Used to convert keypoint coordinates in the raw image to coordinates\n",
    "        # in the confidence maps.\n",
    "        scale = (img.shape[0] / confmap_shape[0], img.shape[1] / confmap_shape[1])\n",
    "\n",
    "        # Build Y (label; ground truth confidence maps)\n",
    "        annos = coco.loadAnns(coco.getAnnIds(imgIds=img_data['id']))\n",
    "        for anno in annos: # each annotation corresponds to a different person\n",
    "            update_confidence_map(img.shape, Y[i,:,:,:], anno['keypoints'], scale, sigma, img_path)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.53s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/COCO/images/train2014/COCO_train2014_000000312958.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a1d44effe1d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dataset/COCO\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train2014'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a9aa712cf165>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_dir, data_type, image_shape, sigma, num_input, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scale pixel value to [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Handle black and white images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                (plugin, kind))\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, dtype, img_num, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/COCO/images/train2014/COCO_train2014_000000312958.jpg'"
     ]
    }
   ],
   "source": [
    "X, Y = load_data(\"../dataset/COCO\", 'train2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.transpose((1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
